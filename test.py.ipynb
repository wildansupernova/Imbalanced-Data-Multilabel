{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotions:train - exists, not redownloading\n",
      "emotions:test - exists, not redownloading\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.dataset import load_dataset\n",
    "X_train, y_train, feature_names, label_names = load_dataset('emotions', 'train')\n",
    "X_test, y_test, _, _ = load_dataset('emotions', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import hstack, issparse, lil_matrix\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "class BinaryRelevanceUnderSampling(BinaryRelevance):\n",
    "    def __init__(self, classifier=None, require_dense=None, seed=1):\n",
    "        super(BinaryRelevanceUnderSampling, self).__init__(\n",
    "            classifier, require_dense)\n",
    "        self.seed = seed\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fits classifier to training data\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : `array_like`, :class:`numpy.matrix` or :mod:`scipy.sparse` matrix, shape=(n_samples, n_features)\n",
    "            input feature matrix\n",
    "        y : `array_like`, :class:`numpy.matrix` or :mod:`scipy.sparse` matrix of `{0, 1}`, shape=(n_samples, n_labels)\n",
    "            binary indicator matrix with label assignments\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "            fitted instance of self\n",
    "        Notes\n",
    "        -----\n",
    "        .. note :: Input matrices are converted to sparse format internally if a numpy representation is passed\n",
    "        \"\"\"\n",
    "        X = self._ensure_input_format(\n",
    "            X, sparse_format='csr', enforce_sparse=True)\n",
    "        y = self._ensure_output_format(\n",
    "            y, sparse_format='csc', enforce_sparse=True)\n",
    "\n",
    "        self.classifiers_ = []\n",
    "        self._generate_partition(X, y)\n",
    "        self._label_count = y.shape[1]\n",
    "\n",
    "        rus = RandomUnderSampler(random_state=self.seed)\n",
    "\n",
    "        for i in range(self.model_count_):\n",
    "            print(\"Binary Relevance: Creating Model \", i)\n",
    "            classifier = copy.deepcopy(self.classifier)\n",
    "            y_subset = self._generate_data_subset(\n",
    "                y, self.partition_[i], axis=1)\n",
    "            y_to_resample = y_subset.toarray()\n",
    "            flattenVer = y_to_resample.flatten()\n",
    "            uniqueElementLength = len(np.unique(flattenVer))\n",
    "            if uniqueElementLength > 1:\n",
    "                X_resampled, y_resampled = rus.fit_resample(X,y_to_resample)\n",
    "            else:\n",
    "                X_resampled, y_resampled = X, y_to_resample\n",
    "            if issparse(y_resampled) and y_resampled.ndim > 1 and y_resampled.shape[1] == 1:\n",
    "                y_resampled = np.ravel(y_resampled.toarray())\n",
    "            classifier.fit(self._ensure_input_format(\n",
    "                X_resampled), self._ensure_output_format(y_resampled))\n",
    "            self.classifiers_.append(classifier)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.base import ProblemTransformationBase\n",
    "# from BinaryRelevanceUnderSampling import BinaryRelevanceUnderSampling\n",
    "from CocoaTripleClassTransformation import CocoaTripleClassTransformation\n",
    "from imblearn.datasets import make_imbalance\n",
    "from sklearn.metrics import f1_score\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "\n",
    "paramXGBoost = {\n",
    "    \"eta\":0.2,\n",
    "    \"gamma\":0,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"max_depth\": 3,\n",
    "    \"colsample_bytree\": 0.7\n",
    "}\n",
    "\n",
    "paramXGBoostMulticlass = {\n",
    "    \"eta\":0.2,\n",
    "    \"gamma\":0,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"max_depth\": 3,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"objective\": \"multi:softmax\"\n",
    "}\n",
    "\n",
    "\n",
    "class CocoaXGBoostUndersampling(ProblemTransformationBase):\n",
    "    def __init__(self, numMaxCouples = 10, underSamplingPercent = 1.0, seed = 1):\n",
    "        super(CocoaXGBoostUndersampling, self).__init__(XGBClassifier(**paramXGBoost), None)\n",
    "        self.multiclassClassifier = XGBClassifier(**paramXGBoostMulticlass)\n",
    "        self.numMaxCouples = numMaxCouples\n",
    "        self.underSamplingPercent = underSamplingPercent\n",
    "        self.seed = seed\n",
    "        self.numCouples = None\n",
    "\n",
    "    def getNumMaxCouples(self):\n",
    "        return self.numMaxCouples\n",
    "    \n",
    "    def getNumCouples(self):\n",
    "        return self.numCouples\n",
    "    \n",
    "    def getUnderSamplingPercent(self):\n",
    "        return self.underSamplingPercent\n",
    "\n",
    "    def setUnderSamplingPercent(self, underSamplingPercent):\n",
    "        self.underSamplingPercent = underSamplingPercent\n",
    "    \n",
    "    def getSeed(self):\n",
    "        return self.seed\n",
    "\n",
    "    def setSeed(self, seed):\n",
    "        self.seed = seed\n",
    "\n",
    "    def _generate_partition(self, X, y):\n",
    "        \"\"\"Partitions the label space into singletons\n",
    "        Sets `self.partition_` (list of single item lists) and `self.model_count_` (equal to number of labels).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : `array_like`, :class:`numpy.matrix` or :mod:`scipy.sparse` matrix, shape=(n_samples, n_features)\n",
    "            not used, only for API compatibility\n",
    "        y : `array_like`, :class:`numpy.matrix` or :mod:`scipy.sparse` matrix of `int`, shape=(n_samples, n_labels)\n",
    "            binary indicator matrix with label assignments\n",
    "        \"\"\"\n",
    "        self.partition_ = list(range(y.shape[1]))\n",
    "        self.labelIndices = list(range(y.shape[1]))\n",
    "        self.model_count_ = y.shape[1]\n",
    "        self._label_count = y.shape[1]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fits classifier to training data\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : `array_like`, :class:`numpy.matrix` or :mod:`scipy.sparse` matrix, shape=(n_samples, n_features)\n",
    "            input feature matrix\n",
    "        y : `array_like`, :class:`numpy.matrix` or :mod:`scipy.sparse` matrix of `{0, 1}`, shape=(n_samples, n_labels)\n",
    "            binary indicator matrix with label assignments\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "            fitted instance of self\n",
    "        Notes\n",
    "        -----\n",
    "        .. note :: Input matrices are converted to sparse format internally if a numpy representation is passed\n",
    "        \"\"\"\n",
    "        self._generate_partition(X, y)\n",
    "        self.numCouples = min(self.getNumMaxCouples(), self._label_count-1)\n",
    "        self.brus = BinaryRelevanceUnderSampling(self.classifier, seed=self.seed)\n",
    "        self.trt = CocoaTripleClassTransformation(y)\n",
    "        self.triLabelIndices = []\n",
    "        self.triClassifiers = []\n",
    "        for i in range(self._label_count):\n",
    "            self.triLabelIndices.append([]) # Actually init indices\n",
    "            self.triClassifiers.append([]) # Actually init classifier\n",
    "            for j in range(self.numCouples):\n",
    "                self.triClassifiers[i].append(copy.deepcopy(self.multiclassClassifier)) \n",
    "        self.thresholds = [-1]*self._label_count #Init threshold list\n",
    "        \n",
    "        self.brus.fit(X, y)\n",
    "\t\t\n",
    "        labelIndexList = []\n",
    "        for i in range(self._label_count):\n",
    "            labelIndexList.append(self.labelIndices[i])\n",
    "\n",
    "        rnd = random.Random(self.seed)\n",
    "        for i in range(self._label_count):\n",
    "            rnd.shuffle(labelIndexList)\n",
    "            self.triLabelIndices[i] = self.selectedLabelIndices(labelIndexList, self.labelIndices[i])\n",
    "            for j in range(self.numCouples):\n",
    "                print(\"Coupling: \", i, \" and \", j)\n",
    "                yTriClassIns = self.trt.transformLabels(self.labelIndices[i], self.triLabelIndices[i][j])\n",
    "                xUsTriClassIns, YUsTriClassIns = self.TrirandomUnderSampling(X, yTriClassIns)\n",
    "                self.triClassifiers[i][j].fit(xUsTriClassIns, YUsTriClassIns)\n",
    "       \tself.calculateThresholds(X, y)\n",
    "\n",
    "    def selectedLabelIndices(self, labelIndexList, currentLabelIndex):\n",
    "        result = []\n",
    "        i_list = 0\n",
    "        i_array = 0\n",
    "        while i_array<self.numCouples:\n",
    "            l=labelIndexList[i_list]\n",
    "            if l!=currentLabelIndex:\n",
    "                result.append(l)\n",
    "                i_array+=1\n",
    "            i_list+=1\n",
    "        return result\n",
    "\n",
    "    def TrirandomUnderSampling(self, X, y): \n",
    "        \"\"\"\n",
    "        y : numpy array\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "#         dictCount = dict(zip(unique_elements, counts_elements))\n",
    "        numClass = len(unique_elements)\n",
    "        c = [0]*numClass\n",
    "        nData = len(y)\n",
    "        minVal = counts_elements.min()\n",
    "        sample_strategy = dict()\n",
    "        for i in range(numClass):\n",
    "            if i in unique_elements:\n",
    "                sample_strategy[i] = minVal\n",
    "        Xres, yres = make_imbalance(X,y,sample_strategy)\n",
    "        return Xres, yres\n",
    "\n",
    "    def makePredictionforThreshold(self, xData):\n",
    "        confidences = [0]*self._label_count\n",
    "        X = np.asarray([xData])\n",
    "        yPredProba = self.brus.predict_proba(X).toarray()[0]\n",
    "\n",
    "        for i in range(self._label_count):\n",
    "            confidences+=yPredProba[i]\n",
    "\n",
    "        for j in range(self._label_count):\n",
    "            for k in range(self.numCouples):\n",
    "                d = self.triClassifiers[j][k].predict_proba(np.asarray([xData]))\n",
    "                confidences[j] += d[0][2]\n",
    "            confidences[j] /= (self.numCouples+1)\n",
    "        return confidences\n",
    "\n",
    "    def calculateThresholds(self, X, y):\n",
    "        nData = y.shape[0]\n",
    "        nLabel = y.shape[1]\n",
    "        predictConfidences = []\n",
    "        for i in range(nData):\n",
    "            predictConfidences.append(self.makePredictionforThreshold(X[i]))\n",
    "\n",
    "        for j in range(self._label_count):\n",
    "            maxVal = -1000000000000.0\n",
    "            trueLabels = [ data[j]==1 for data in y]\n",
    "            \n",
    "            d = 0.05\n",
    "\n",
    "            while d<1:\n",
    "                predictLabels = [predictConfidences[i][j]>=d for i in list(range(nData))]\n",
    "                #Using Fmeasure\n",
    "                print(\"Calculate Threshold Label \", j, \"with d=\",d)\n",
    "                value = f1_score(trueLabels, predictLabels, average='macro')\n",
    "                if value > maxVal:\n",
    "                    maxVal = value\n",
    "                    self.thresholds[j] = d\n",
    "                d+=0.05 \n",
    "\n",
    "    def predict(self, X):\n",
    "        nData = len(X)\n",
    "        result = []\n",
    "        for i in range(nData):\n",
    "            bipartition, confidences = self.makePredictionSingleData(X[i])\n",
    "            result.append(bipartition)\n",
    "        return np.asarray(result)\n",
    "\n",
    "    def makePredictionSingleData(self, x1):\n",
    "        confidences = self.makePredictionforThreshold(x1)\n",
    "        bipartition = [0]*self._label_count\n",
    "        for j in range(self._label_count):\n",
    "            bipartition[j] = int(confidences[j] > self.thresholds[j])\n",
    "\n",
    "        return bipartition, confidences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = CocoaXGBoostUndersampling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Relevance: Creating Model  0\n",
      "Binary Relevance: Creating Model  1\n",
      "Binary Relevance: Creating Model  2\n",
      "Binary Relevance: Creating Model  3\n",
      "Binary Relevance: Creating Model  4\n",
      "Binary Relevance: Creating Model  5\n",
      "Coupling:  0  and  0\n",
      "Coupling:  0  and  1\n",
      "Coupling:  0  and  2\n",
      "Coupling:  0  and  3\n",
      "Coupling:  0  and  4\n",
      "Coupling:  1  and  0\n",
      "Coupling:  1  and  1\n",
      "Coupling:  1  and  2\n",
      "Coupling:  1  and  3\n",
      "Coupling:  1  and  4\n",
      "Coupling:  2  and  0\n",
      "Coupling:  2  and  1\n",
      "Coupling:  2  and  2\n",
      "Coupling:  2  and  3\n",
      "Coupling:  2  and  4\n",
      "Coupling:  3  and  0\n",
      "Coupling:  3  and  1\n",
      "Coupling:  3  and  2\n",
      "Coupling:  3  and  3\n",
      "Coupling:  3  and  4\n",
      "Coupling:  4  and  0\n",
      "Coupling:  4  and  1\n",
      "Coupling:  4  and  2\n",
      "Coupling:  4  and  3\n",
      "Coupling:  4  and  4\n",
      "Coupling:  5  and  0\n",
      "Coupling:  5  and  1\n",
      "Coupling:  5  and  2\n",
      "Coupling:  5  and  3\n",
      "Coupling:  5  and  4\n",
      "Calculate Threshold Label  0 with d= 0.05\n",
      "Calculate Threshold Label  0 with d= 0.1\n",
      "Calculate Threshold Label  0 with d= 0.15000000000000002\n",
      "Calculate Threshold Label  0 with d= 0.2\n",
      "Calculate Threshold Label  0 with d= 0.25\n",
      "Calculate Threshold Label  0 with d= 0.3\n",
      "Calculate Threshold Label  0 with d= 0.35\n",
      "Calculate Threshold Label  0 with d= 0.39999999999999997\n",
      "Calculate Threshold Label  0 with d= 0.44999999999999996\n",
      "Calculate Threshold Label  0 with d= 0.49999999999999994\n",
      "Calculate Threshold Label  0 with d= 0.5499999999999999\n",
      "Calculate Threshold Label  0 with d= 0.6\n",
      "Calculate Threshold Label  0 with d= 0.65\n",
      "Calculate Threshold Label  0 with d= 0.7000000000000001\n",
      "Calculate Threshold Label  0 with d= 0.7500000000000001\n",
      "Calculate Threshold Label  0 with d= 0.8000000000000002\n",
      "Calculate Threshold Label  0 with d= 0.8500000000000002\n",
      "Calculate Threshold Label  0 with d= 0.9000000000000002\n",
      "Calculate Threshold Label  0 with d= 0.9500000000000003\n",
      "Calculate Threshold Label  1 with d= 0.05\n",
      "Calculate Threshold Label  1 with d= 0.1\n",
      "Calculate Threshold Label  1 with d= 0.15000000000000002\n",
      "Calculate Threshold Label  1 with d= 0.2\n",
      "Calculate Threshold Label  1 with d= 0.25\n",
      "Calculate Threshold Label  1 with d= 0.3\n",
      "Calculate Threshold Label  1 with d= 0.35\n",
      "Calculate Threshold Label  1 with d= 0.39999999999999997\n",
      "Calculate Threshold Label  1 with d= 0.44999999999999996\n",
      "Calculate Threshold Label  1 with d= 0.49999999999999994\n",
      "Calculate Threshold Label  1 with d= 0.5499999999999999\n",
      "Calculate Threshold Label  1 with d= 0.6\n",
      "Calculate Threshold Label  1 with d= 0.65\n",
      "Calculate Threshold Label  1 with d= 0.7000000000000001\n",
      "Calculate Threshold Label  1 with d= 0.7500000000000001\n",
      "Calculate Threshold Label  1 with d= 0.8000000000000002\n",
      "Calculate Threshold Label  1 with d= 0.8500000000000002\n",
      "Calculate Threshold Label  1 with d= 0.9000000000000002\n",
      "Calculate Threshold Label  1 with d= 0.9500000000000003\n",
      "Calculate Threshold Label  2 with d= 0.05\n",
      "Calculate Threshold Label  2 with d= 0.1\n",
      "Calculate Threshold Label  2 with d= 0.15000000000000002\n",
      "Calculate Threshold Label  2 with d= 0.2\n",
      "Calculate Threshold Label  2 with d= 0.25\n",
      "Calculate Threshold Label  2 with d= 0.3\n",
      "Calculate Threshold Label  2 with d= 0.35\n",
      "Calculate Threshold Label  2 with d= 0.39999999999999997\n",
      "Calculate Threshold Label  2 with d= 0.44999999999999996\n",
      "Calculate Threshold Label  2 with d= 0.49999999999999994\n",
      "Calculate Threshold Label  2 with d= 0.5499999999999999\n",
      "Calculate Threshold Label  2 with d= 0.6\n",
      "Calculate Threshold Label  2 with d= 0.65\n",
      "Calculate Threshold Label  2 with d= 0.7000000000000001\n",
      "Calculate Threshold Label  2 with d= 0.7500000000000001\n",
      "Calculate Threshold Label  2 with d= 0.8000000000000002\n",
      "Calculate Threshold Label  2 with d= 0.8500000000000002\n",
      "Calculate Threshold Label  2 with d= 0.9000000000000002\n",
      "Calculate Threshold Label  2 with d= 0.9500000000000003\n",
      "Calculate Threshold Label  3 with d= 0.05\n",
      "Calculate Threshold Label  3 with d= 0.1\n",
      "Calculate Threshold Label  3 with d= 0.15000000000000002\n",
      "Calculate Threshold Label  3 with d= 0.2\n",
      "Calculate Threshold Label  3 with d= 0.25\n",
      "Calculate Threshold Label  3 with d= 0.3\n",
      "Calculate Threshold Label  3 with d= 0.35\n",
      "Calculate Threshold Label  3 with d= 0.39999999999999997\n",
      "Calculate Threshold Label  3 with d= 0.44999999999999996\n",
      "Calculate Threshold Label  3 with d= 0.49999999999999994\n",
      "Calculate Threshold Label  3 with d= 0.5499999999999999\n",
      "Calculate Threshold Label  3 with d= 0.6\n",
      "Calculate Threshold Label  3 with d= 0.65\n",
      "Calculate Threshold Label  3 with d= 0.7000000000000001\n",
      "Calculate Threshold Label  3 with d= 0.7500000000000001\n",
      "Calculate Threshold Label  3 with d= 0.8000000000000002\n",
      "Calculate Threshold Label  3 with d= 0.8500000000000002\n",
      "Calculate Threshold Label  3 with d= 0.9000000000000002\n",
      "Calculate Threshold Label  3 with d= 0.9500000000000003\n",
      "Calculate Threshold Label  4 with d= 0.05\n",
      "Calculate Threshold Label  4 with d= 0.1\n",
      "Calculate Threshold Label  4 with d= 0.15000000000000002\n",
      "Calculate Threshold Label  4 with d= 0.2\n",
      "Calculate Threshold Label  4 with d= 0.25\n",
      "Calculate Threshold Label  4 with d= 0.3\n",
      "Calculate Threshold Label  4 with d= 0.35\n",
      "Calculate Threshold Label  4 with d= 0.39999999999999997\n",
      "Calculate Threshold Label  4 with d= 0.44999999999999996\n",
      "Calculate Threshold Label  4 with d= 0.49999999999999994\n",
      "Calculate Threshold Label  4 with d= 0.5499999999999999\n",
      "Calculate Threshold Label  4 with d= 0.6\n",
      "Calculate Threshold Label  4 with d= 0.65\n",
      "Calculate Threshold Label  4 with d= 0.7000000000000001\n",
      "Calculate Threshold Label  4 with d= 0.7500000000000001\n",
      "Calculate Threshold Label  4 with d= 0.8000000000000002\n",
      "Calculate Threshold Label  4 with d= 0.8500000000000002\n",
      "Calculate Threshold Label  4 with d= 0.9000000000000002\n",
      "Calculate Threshold Label  4 with d= 0.9500000000000003\n",
      "Calculate Threshold Label  5 with d= 0.05\n",
      "Calculate Threshold Label  5 with d= 0.1\n",
      "Calculate Threshold Label  5 with d= 0.15000000000000002\n",
      "Calculate Threshold Label  5 with d= 0.2\n",
      "Calculate Threshold Label  5 with d= 0.25\n",
      "Calculate Threshold Label  5 with d= 0.3\n",
      "Calculate Threshold Label  5 with d= 0.35\n",
      "Calculate Threshold Label  5 with d= 0.39999999999999997\n",
      "Calculate Threshold Label  5 with d= 0.44999999999999996\n",
      "Calculate Threshold Label  5 with d= 0.49999999999999994\n",
      "Calculate Threshold Label  5 with d= 0.5499999999999999\n",
      "Calculate Threshold Label  5 with d= 0.6\n",
      "Calculate Threshold Label  5 with d= 0.65\n",
      "Calculate Threshold Label  5 with d= 0.7000000000000001\n",
      "Calculate Threshold Label  5 with d= 0.7500000000000001\n",
      "Calculate Threshold Label  5 with d= 0.8000000000000002\n",
      "Calculate Threshold Label  5 with d= 0.8500000000000002\n",
      "Calculate Threshold Label  5 with d= 0.9000000000000002\n",
      "Calculate Threshold Label  5 with d= 0.9500000000000003\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train.toarray(), y_train.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = clf.predict(X_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2070957095709571"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, hamming_loss, f1_score\n",
    "hamloss = hamming_loss(y_test,ypred)\n",
    "hamloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# initialize Binary Relevance multi-label classifier\n",
    "# with an SVM classifier\n",
    "# SVM in scikit only supports the X matrix in sparse representation\n",
    "\n",
    "classifier = BinaryRelevance(\n",
    "    classifier = XGBClassifier(**paramXGBoost),\n",
    "    require_dense = [False, True]\n",
    ")\n",
    "\n",
    "# train\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20544554455445543"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamloss = hamming_loss(y_test,predictions)\n",
    "hamloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6600    0.6111    0.6346        54\n",
      "           1     0.6000    0.3559    0.4468        59\n",
      "           2     0.6827    0.7396    0.7100        96\n",
      "           3     0.8364    0.7797    0.8070        59\n",
      "           4     0.8478    0.5342    0.6555        73\n",
      "           5     0.7500    0.5172    0.6122        58\n",
      "\n",
      "   micro avg     0.7273    0.6015    0.6584       399\n",
      "   macro avg     0.7295    0.5896    0.6444       399\n",
      "weighted avg     0.7301    0.6015    0.6510       399\n",
      " samples avg     0.6568    0.5965    0.5931       399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6600    0.6111    0.6346        54\n",
      "           1     0.4915    0.4915    0.4915        59\n",
      "           2     0.6796    0.7292    0.7035        96\n",
      "           3     0.7424    0.8305    0.7840        59\n",
      "           4     0.8070    0.6301    0.7077        73\n",
      "           5     0.7959    0.6724    0.7290        58\n",
      "\n",
      "   micro avg     0.6927    0.6667    0.6794       399\n",
      "   macro avg     0.6961    0.6608    0.6751       399\n",
      "weighted avg     0.6986    0.6667    0.6792       399\n",
      " samples avg     0.6502    0.6617    0.6254       399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,ypred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "X_res, y_res = make_imbalance(X, y,sampling_strategy={0: 10, 1: 20, 2: 30},random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.under_sampling import RandomUnderSampler # doctest: +NORMALIZE_WHITESPACE\n",
    "X, y = make_classification(n_classes=2, class_sep=2,weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
    "y2 = [[d] for d in y]\n",
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rus = RandomUnderSampler(random_state=42)\n",
    "X1, y1 = rus.fit_resample(X,y2)\n",
    "np.unique(y1.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1, y1 = rus.fit_resample(X,y)\n",
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[0], [1], [2], [3]]\n",
    "y = [0, 0, 1, 1]\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 1., 1.]]), array([[1, 0, 2]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.kneighbors([[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.predict([[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
