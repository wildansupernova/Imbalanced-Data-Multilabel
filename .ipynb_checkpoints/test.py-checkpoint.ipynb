{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotions:train - exists, not redownloading\n",
      "emotions:test - exists, not redownloading\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.dataset import load_dataset\n",
    "X_train, y_train, feature_names, label_names = load_dataset('emotions', 'train')\n",
    "X_test, y_test, _, _ = load_dataset('emotions', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import hstack, issparse, lil_matrix\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "class BinaryRelevanceUnderSampling(BinaryRelevance):\n",
    "    def __init__(self, classifier=None, require_dense=None, seed=1):\n",
    "        super(BinaryRelevanceUnderSampling, self).__init__(\n",
    "            classifier, require_dense)\n",
    "        self.seed = seed\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fits classifier to training data\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : `array_like`, :class:`numpy.matrix` or :mod:`scipy.sparse` matrix, shape=(n_samples, n_features)\n",
    "            input feature matrix\n",
    "        y : `array_like`, :class:`numpy.matrix` or :mod:`scipy.sparse` matrix of `{0, 1}`, shape=(n_samples, n_labels)\n",
    "            binary indicator matrix with label assignments\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "            fitted instance of self\n",
    "        Notes\n",
    "        -----\n",
    "        .. note :: Input matrices are converted to sparse format internally if a numpy representation is passed\n",
    "        \"\"\"\n",
    "        X = self._ensure_input_format(\n",
    "            X, sparse_format='csr', enforce_sparse=True)\n",
    "        y = self._ensure_output_format(\n",
    "            y, sparse_format='csc', enforce_sparse=True)\n",
    "\n",
    "        self.classifiers_ = []\n",
    "        self._generate_partition(X, y)\n",
    "        self._label_count = y.shape[1]\n",
    "\n",
    "        rus = RandomUnderSampler(random_state=self.seed)\n",
    "\n",
    "        for i in range(self.model_count_):\n",
    "            print(\"Binary Relevance: Creating Model \", i)\n",
    "            classifier = copy.deepcopy(self.classifier)\n",
    "            y_subset = self._generate_data_subset(\n",
    "                y, self.partition_[i], axis=1)\n",
    "            y_to_resample = y_subset.toarray()\n",
    "            flattenVer = y_to_resample.flatten()\n",
    "            uniqueElementLength = len(np.unique(flattenVer))\n",
    "            if uniqueElementLength > 1:\n",
    "                X_resampled, y_resampled = rus.fit_resample(X,y_to_resample)\n",
    "            else:\n",
    "                X_resampled, y_resampled = X, y_to_resample\n",
    "            if issparse(y_resampled) and y_resampled.ndim > 1 and y_resampled.shape[1] == 1:\n",
    "                y_resampled = np.ravel(y_resampled.toarray())\n",
    "            classifier.fit(self._ensure_input_format(\n",
    "                X_resampled), self._ensure_output_format(y_resampled))\n",
    "            self.classifiers_.append(classifier)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.base import ProblemTransformationBase\n",
    "# from BinaryRelevanceUnderSampling import BinaryRelevanceUnderSampling\n",
    "from CocoaTripleClassTransformation import CocoaTripleClassTransformation\n",
    "from imblearn.datasets import make_imbalance\n",
    "from sklearn.metrics import f1_score\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "\n",
    "paramXGBoost = {\n",
    "    \"eta\":0.2,\n",
    "    \"gamma\":0,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"max_depth\": 3,\n",
    "    \"colsample_bytree\": 0.7\n",
    "}\n",
    "\n",
    "paramXGBoostMulticlass = {\n",
    "    \"eta\":0.2,\n",
    "    \"gamma\":0,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"max_depth\": 3,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"objective\": \"multi:softmax\"\n",
    "}\n",
    "\n",
    "\n",
    "class CocoaXGBoostUndersampling(ProblemTransformationBase):\n",
    "    def __init__(self, numMaxCouples = 10, underSamplingPercent = 1.0, seed = 1):\n",
    "        super(CocoaXGBoostUndersampling, self).__init__(XGBClassifier(**paramXGBoost), None)\n",
    "        self.multiclassClassifier = XGBClassifier(**paramXGBoostMulticlass)\n",
    "        self.numMaxCouples = numMaxCouples\n",
    "        self.underSamplingPercent = underSamplingPercent\n",
    "        self.seed = seed\n",
    "        self.numCouples = None\n",
    "\n",
    "    def getNumMaxCouples(self):\n",
    "        return self.numMaxCouples\n",
    "    \n",
    "    def getNumCouples(self):\n",
    "        return self.numCouples\n",
    "    \n",
    "    def getUnderSamplingPercent(self):\n",
    "        return self.underSamplingPercent\n",
    "\n",
    "    def setUnderSamplingPercent(self, underSamplingPercent):\n",
    "        self.underSamplingPercent = underSamplingPercent\n",
    "    \n",
    "    def getSeed(self):\n",
    "        return self.seed\n",
    "\n",
    "    def setSeed(self, seed):\n",
    "        self.seed = seed\n",
    "\n",
    "    def _generate_partition(self, X, y):\n",
    "        \"\"\"Partitions the label space into singletons\n",
    "        Sets `self.partition_` (list of single item lists) and `self.model_count_` (equal to number of labels).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : `array_like`, :class:`numpy.matrix` or :mod:`scipy.sparse` matrix, shape=(n_samples, n_features)\n",
    "            not used, only for API compatibility\n",
    "        y : `array_like`, :class:`numpy.matrix` or :mod:`scipy.sparse` matrix of `int`, shape=(n_samples, n_labels)\n",
    "            binary indicator matrix with label assignments\n",
    "        \"\"\"\n",
    "        self.partition_ = list(range(y.shape[1]))\n",
    "        self.labelIndices = list(range(y.shape[1]))\n",
    "        self.model_count_ = y.shape[1]\n",
    "        self._label_count = y.shape[1]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fits classifier to training data\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : `array_like`, :class:`numpy.matrix` or :mod:`scipy.sparse` matrix, shape=(n_samples, n_features)\n",
    "            input feature matrix\n",
    "        y : `array_like`, :class:`numpy.matrix` or :mod:`scipy.sparse` matrix of `{0, 1}`, shape=(n_samples, n_labels)\n",
    "            binary indicator matrix with label assignments\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "            fitted instance of self\n",
    "        Notes\n",
    "        -----\n",
    "        .. note :: Input matrices are converted to sparse format internally if a numpy representation is passed\n",
    "        \"\"\"\n",
    "        self._generate_partition(X, y)\n",
    "        self.numCouples = min(self.getNumMaxCouples(), self._label_count-1)\n",
    "        self.brus = BinaryRelevanceUnderSampling(self.classifier, seed=self.seed)\n",
    "        self.trt = CocoaTripleClassTransformation(y)\n",
    "        self.triLabelIndices = []\n",
    "        self.triClassifiers = []\n",
    "        for i in range(self._label_count):\n",
    "            self.triLabelIndices.append([]) # Actually init indices\n",
    "            self.triClassifiers.append([]) # Actually init classifier\n",
    "            for j in range(self.numCouples):\n",
    "                self.triClassifiers[i].append(copy.deepcopy(self.multiclassClassifier)) \n",
    "        self.thresholds = [-1]*self._label_count #Init threshold list\n",
    "        \n",
    "        self.brus.fit(X, y)\n",
    "\t\t\n",
    "        labelIndexList = []\n",
    "        for i in range(self._label_count):\n",
    "            labelIndexList.append(self.labelIndices[i])\n",
    "\n",
    "        rnd = random.Random(self.seed)\n",
    "        for i in range(self._label_count):\n",
    "            rnd.shuffle(labelIndexList)\n",
    "            self.triLabelIndices[i] = self.selectedLabelIndices(labelIndexList, self.labelIndices[i])\n",
    "            for j in range(self.numCouples):\n",
    "                print(\"Coupling: \", i, \" and \", j)\n",
    "                yTriClassIns = self.trt.transformLabels(self.labelIndices[i], self.triLabelIndices[i][j])\n",
    "                xUsTriClassIns, YUsTriClassIns = self.TrirandomUnderSampling(X, yTriClassIns)\n",
    "                self.triClassifiers[i][j].fit(xUsTriClassIns, YUsTriClassIns)\n",
    "       \tself.calculateThresholds(X, y)\n",
    "\n",
    "    def selectedLabelIndices(self, labelIndexList, currentLabelIndex):\n",
    "        result = []\n",
    "        i_list = 0\n",
    "        i_array = 0\n",
    "        while i_array<self.numCouples:\n",
    "            l=labelIndexList[i_list]\n",
    "            if l!=currentLabelIndex:\n",
    "                result.append(l)\n",
    "                i_array+=1\n",
    "            i_list+=1\n",
    "        return result\n",
    "\n",
    "    def TrirandomUnderSampling(self, X, y): \n",
    "        \"\"\"\n",
    "        y : numpy array\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "#         dictCount = dict(zip(unique_elements, counts_elements))\n",
    "        numClass = len(unique_elements)\n",
    "        c = [0]*numClass\n",
    "        nData = len(y)\n",
    "        minVal = counts_elements.min()\n",
    "        sample_strategy = dict()\n",
    "        for i in range(numClass):\n",
    "            if i in unique_elements:\n",
    "                sample_strategy[i] = minVal\n",
    "        Xres, yres = make_imbalance(X,y,sample_strategy)\n",
    "        return Xres, yres\n",
    "\n",
    "    def makePredictionforThreshold(self, xData):\n",
    "        confidences = [0]*self._label_count\n",
    "        X = np.asarray([xData])\n",
    "        yPredProba = self.brus.predict_proba(X).toarray()[0]\n",
    "\n",
    "        for i in range(self._label_count):\n",
    "            confidences+=yPredProba[i]\n",
    "\n",
    "        for j in range(self._label_count):\n",
    "            for k in range(self.numCouples):\n",
    "                d = self.triClassifiers[j][k].predict_proba(np.asarray([xData]))\n",
    "                confidences[j] += d[0][2]\n",
    "            confidences[j] /= (self.numCouples+1)\n",
    "        return confidences\n",
    "\n",
    "    def calculateThresholds(self, X, y):\n",
    "        nData = y.shape[0]\n",
    "        nLabel = y.shape[1]\n",
    "        predictConfidences = []\n",
    "        for i in range(nData):\n",
    "            predictConfidences.append(self.makePredictionforThreshold(X[i]))\n",
    "\n",
    "        for j in range(self._label_count):\n",
    "            maxVal = -1000000000000.0\n",
    "            trueLabels = [ data[j]==1 for data in y]\n",
    "            \n",
    "            d = 0.05\n",
    "\n",
    "            while d<1:\n",
    "                predictLabels = [predictConfidences[i][j]>=d for i in list(range(nData))]\n",
    "                #Using Fmeasure\n",
    "                print(\"Calculate Threshold Label \", j, \"with d=\",d)\n",
    "                value = f1_score(trueLabels, predictLabels, average='macro')\n",
    "                if value > maxVal:\n",
    "                    maxVal = value\n",
    "                    self.thresholds[j] = d\n",
    "                d+=0.05 \n",
    "\n",
    "    def predict(self, X):\n",
    "        nData = len(X)\n",
    "        result = []\n",
    "        for i in range(nData):\n",
    "            bipartition, confidences = self.makePredictionSingleData(X[i])\n",
    "            result.append(bipartition)\n",
    "        return np.asarray(result)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        nData = len(X)\n",
    "        result = []\n",
    "        for i in range(nData):\n",
    "            confidences = self.makePredictionforThreshold(X[i])\n",
    "            result.append(confidences)\n",
    "        return np.asarray(result) \n",
    "    \n",
    "    def makePredictionSingleData(self, x1):\n",
    "        confidences = self.makePredictionforThreshold(x1)\n",
    "        bipartition = [0]*self._label_count\n",
    "        for j in range(self._label_count):\n",
    "            bipartition[j] = int(confidences[j] > self.thresholds[j])\n",
    "\n",
    "        return bipartition, confidences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = CocoaXGBoostUndersampling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Relevance: Creating Model  0\n",
      "Binary Relevance: Creating Model  1\n",
      "Binary Relevance: Creating Model  2\n",
      "Binary Relevance: Creating Model  3\n",
      "Binary Relevance: Creating Model  4\n",
      "Binary Relevance: Creating Model  5\n",
      "Coupling:  0  and  0\n",
      "Coupling:  0  and  1\n",
      "Coupling:  0  and  2\n",
      "Coupling:  0  and  3\n",
      "Coupling:  0  and  4\n",
      "Coupling:  1  and  0\n",
      "Coupling:  1  and  1\n",
      "Coupling:  1  and  2\n",
      "Coupling:  1  and  3\n",
      "Coupling:  1  and  4\n",
      "Coupling:  2  and  0\n",
      "Coupling:  2  and  1\n",
      "Coupling:  2  and  2\n",
      "Coupling:  2  and  3\n",
      "Coupling:  2  and  4\n",
      "Coupling:  3  and  0\n",
      "Coupling:  3  and  1\n",
      "Coupling:  3  and  2\n",
      "Coupling:  3  and  3\n",
      "Coupling:  3  and  4\n",
      "Coupling:  4  and  0\n",
      "Coupling:  4  and  1\n",
      "Coupling:  4  and  2\n",
      "Coupling:  4  and  3\n",
      "Coupling:  4  and  4\n",
      "Coupling:  5  and  0\n",
      "Coupling:  5  and  1\n",
      "Coupling:  5  and  2\n",
      "Coupling:  5  and  3\n",
      "Coupling:  5  and  4\n",
      "Calculate Threshold Label  0 with d= 0.05\n",
      "Calculate Threshold Label  0 with d= 0.1\n",
      "Calculate Threshold Label  0 with d= 0.15000000000000002\n",
      "Calculate Threshold Label  0 with d= 0.2\n",
      "Calculate Threshold Label  0 with d= 0.25\n",
      "Calculate Threshold Label  0 with d= 0.3\n",
      "Calculate Threshold Label  0 with d= 0.35\n",
      "Calculate Threshold Label  0 with d= 0.39999999999999997\n",
      "Calculate Threshold Label  0 with d= 0.44999999999999996\n",
      "Calculate Threshold Label  0 with d= 0.49999999999999994\n",
      "Calculate Threshold Label  0 with d= 0.5499999999999999\n",
      "Calculate Threshold Label  0 with d= 0.6\n",
      "Calculate Threshold Label  0 with d= 0.65\n",
      "Calculate Threshold Label  0 with d= 0.7000000000000001\n",
      "Calculate Threshold Label  0 with d= 0.7500000000000001\n",
      "Calculate Threshold Label  0 with d= 0.8000000000000002\n",
      "Calculate Threshold Label  0 with d= 0.8500000000000002\n",
      "Calculate Threshold Label  0 with d= 0.9000000000000002\n",
      "Calculate Threshold Label  0 with d= 0.9500000000000003\n",
      "Calculate Threshold Label  1 with d= 0.05\n",
      "Calculate Threshold Label  1 with d= 0.1\n",
      "Calculate Threshold Label  1 with d= 0.15000000000000002\n",
      "Calculate Threshold Label  1 with d= 0.2\n",
      "Calculate Threshold Label  1 with d= 0.25\n",
      "Calculate Threshold Label  1 with d= 0.3\n",
      "Calculate Threshold Label  1 with d= 0.35\n",
      "Calculate Threshold Label  1 with d= 0.39999999999999997\n",
      "Calculate Threshold Label  1 with d= 0.44999999999999996\n",
      "Calculate Threshold Label  1 with d= 0.49999999999999994\n",
      "Calculate Threshold Label  1 with d= 0.5499999999999999\n",
      "Calculate Threshold Label  1 with d= 0.6\n",
      "Calculate Threshold Label  1 with d= 0.65\n",
      "Calculate Threshold Label  1 with d= 0.7000000000000001\n",
      "Calculate Threshold Label  1 with d= 0.7500000000000001\n",
      "Calculate Threshold Label  1 with d= 0.8000000000000002\n",
      "Calculate Threshold Label  1 with d= 0.8500000000000002\n",
      "Calculate Threshold Label  1 with d= 0.9000000000000002\n",
      "Calculate Threshold Label  1 with d= 0.9500000000000003\n",
      "Calculate Threshold Label  2 with d= 0.05\n",
      "Calculate Threshold Label  2 with d= 0.1\n",
      "Calculate Threshold Label  2 with d= 0.15000000000000002\n",
      "Calculate Threshold Label  2 with d= 0.2\n",
      "Calculate Threshold Label  2 with d= 0.25\n",
      "Calculate Threshold Label  2 with d= 0.3\n",
      "Calculate Threshold Label  2 with d= 0.35\n",
      "Calculate Threshold Label  2 with d= 0.39999999999999997\n",
      "Calculate Threshold Label  2 with d= 0.44999999999999996\n",
      "Calculate Threshold Label  2 with d= 0.49999999999999994\n",
      "Calculate Threshold Label  2 with d= 0.5499999999999999\n",
      "Calculate Threshold Label  2 with d= 0.6\n",
      "Calculate Threshold Label  2 with d= 0.65\n",
      "Calculate Threshold Label  2 with d= 0.7000000000000001\n",
      "Calculate Threshold Label  2 with d= 0.7500000000000001\n",
      "Calculate Threshold Label  2 with d= 0.8000000000000002\n",
      "Calculate Threshold Label  2 with d= 0.8500000000000002\n",
      "Calculate Threshold Label  2 with d= 0.9000000000000002\n",
      "Calculate Threshold Label  2 with d= 0.9500000000000003\n",
      "Calculate Threshold Label  3 with d= 0.05\n",
      "Calculate Threshold Label  3 with d= 0.1\n",
      "Calculate Threshold Label  3 with d= 0.15000000000000002\n",
      "Calculate Threshold Label  3 with d= 0.2\n",
      "Calculate Threshold Label  3 with d= 0.25\n",
      "Calculate Threshold Label  3 with d= 0.3\n",
      "Calculate Threshold Label  3 with d= 0.35\n",
      "Calculate Threshold Label  3 with d= 0.39999999999999997\n",
      "Calculate Threshold Label  3 with d= 0.44999999999999996\n",
      "Calculate Threshold Label  3 with d= 0.49999999999999994\n",
      "Calculate Threshold Label  3 with d= 0.5499999999999999\n",
      "Calculate Threshold Label  3 with d= 0.6\n",
      "Calculate Threshold Label  3 with d= 0.65\n",
      "Calculate Threshold Label  3 with d= 0.7000000000000001\n",
      "Calculate Threshold Label  3 with d= 0.7500000000000001\n",
      "Calculate Threshold Label  3 with d= 0.8000000000000002\n",
      "Calculate Threshold Label  3 with d= 0.8500000000000002\n",
      "Calculate Threshold Label  3 with d= 0.9000000000000002\n",
      "Calculate Threshold Label  3 with d= 0.9500000000000003\n",
      "Calculate Threshold Label  4 with d= 0.05\n",
      "Calculate Threshold Label  4 with d= 0.1\n",
      "Calculate Threshold Label  4 with d= 0.15000000000000002\n",
      "Calculate Threshold Label  4 with d= 0.2\n",
      "Calculate Threshold Label  4 with d= 0.25\n",
      "Calculate Threshold Label  4 with d= 0.3\n",
      "Calculate Threshold Label  4 with d= 0.35\n",
      "Calculate Threshold Label  4 with d= 0.39999999999999997\n",
      "Calculate Threshold Label  4 with d= 0.44999999999999996\n",
      "Calculate Threshold Label  4 with d= 0.49999999999999994\n",
      "Calculate Threshold Label  4 with d= 0.5499999999999999\n",
      "Calculate Threshold Label  4 with d= 0.6\n",
      "Calculate Threshold Label  4 with d= 0.65\n",
      "Calculate Threshold Label  4 with d= 0.7000000000000001\n",
      "Calculate Threshold Label  4 with d= 0.7500000000000001\n",
      "Calculate Threshold Label  4 with d= 0.8000000000000002\n",
      "Calculate Threshold Label  4 with d= 0.8500000000000002\n",
      "Calculate Threshold Label  4 with d= 0.9000000000000002\n",
      "Calculate Threshold Label  4 with d= 0.9500000000000003\n",
      "Calculate Threshold Label  5 with d= 0.05\n",
      "Calculate Threshold Label  5 with d= 0.1\n",
      "Calculate Threshold Label  5 with d= 0.15000000000000002\n",
      "Calculate Threshold Label  5 with d= 0.2\n",
      "Calculate Threshold Label  5 with d= 0.25\n",
      "Calculate Threshold Label  5 with d= 0.3\n",
      "Calculate Threshold Label  5 with d= 0.35\n",
      "Calculate Threshold Label  5 with d= 0.39999999999999997\n",
      "Calculate Threshold Label  5 with d= 0.44999999999999996\n",
      "Calculate Threshold Label  5 with d= 0.49999999999999994\n",
      "Calculate Threshold Label  5 with d= 0.5499999999999999\n",
      "Calculate Threshold Label  5 with d= 0.6\n",
      "Calculate Threshold Label  5 with d= 0.65\n",
      "Calculate Threshold Label  5 with d= 0.7000000000000001\n",
      "Calculate Threshold Label  5 with d= 0.7500000000000001\n",
      "Calculate Threshold Label  5 with d= 0.8000000000000002\n",
      "Calculate Threshold Label  5 with d= 0.8500000000000002\n",
      "Calculate Threshold Label  5 with d= 0.9000000000000002\n",
      "Calculate Threshold Label  5 with d= 0.9500000000000003\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train.toarray(), y_train.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = clf.predict(X_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ypred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-ed315bf58f67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhamming_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhamloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhamming_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mypred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mhamloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ypred' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, hamming_loss, f1_score\n",
    "hamloss = hamming_loss(y_test,ypred)\n",
    "hamloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# initialize Binary Relevance multi-label classifier\n",
    "# with an SVM classifier\n",
    "# SVM in scikit only supports the X matrix in sparse representation\n",
    "\n",
    "classifier = BinaryRelevance(\n",
    "    classifier = XGBClassifier(**paramXGBoost),\n",
    "    require_dense = [False, True]\n",
    ")\n",
    "\n",
    "# train\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20544554455445543"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamloss = hamming_loss(y_test,predictions)\n",
    "hamloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6600    0.6111    0.6346        54\n",
      "           1     0.6000    0.3559    0.4468        59\n",
      "           2     0.6827    0.7396    0.7100        96\n",
      "           3     0.8364    0.7797    0.8070        59\n",
      "           4     0.8478    0.5342    0.6555        73\n",
      "           5     0.7500    0.5172    0.6122        58\n",
      "\n",
      "   micro avg     0.7273    0.6015    0.6584       399\n",
      "   macro avg     0.7295    0.5896    0.6444       399\n",
      "weighted avg     0.7301    0.6015    0.6510       399\n",
      " samples avg     0.6568    0.5965    0.5931       399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6600    0.6111    0.6346        54\n",
      "           1     0.4915    0.4915    0.4915        59\n",
      "           2     0.6796    0.7292    0.7035        96\n",
      "           3     0.7424    0.8305    0.7840        59\n",
      "           4     0.8070    0.6301    0.7077        73\n",
      "           5     0.7959    0.6724    0.7290        58\n",
      "\n",
      "   micro avg     0.6927    0.6667    0.6794       399\n",
      "   macro avg     0.6961    0.6608    0.6751       399\n",
      "weighted avg     0.6986    0.6667    0.6792       399\n",
      " samples avg     0.6502    0.6617    0.6254       399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,ypred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "X_res, y_res = make_imbalance(X, y,sampling_strategy={0: 10, 1: 20, 2: 30},random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.under_sampling import RandomUnderSampler # doctest: +NORMALIZE_WHITESPACE\n",
    "X, y = make_classification(n_classes=2, class_sep=2,weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
    "y2 = [[d] for d in y]\n",
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rus = RandomUnderSampler(random_state=42)\n",
    "X1, y1 = rus.fit_resample(X,y2)\n",
    "np.unique(y1.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1, y1 = rus.fit_resample(X,y)\n",
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='brute', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[0], [1], [2], [3]]\n",
    "y = [0, 0, 1, 1]\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3,algorithm='brute')\n",
    "neigh.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 3]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(neigh.kneighbors([[2]], return_distance=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.predict([[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling.base import BaseOverSampler\n",
    "import random\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "class InstanceType:\n",
    "    SAFE =  0\n",
    "    BORDERLINE = 1\n",
    "    RARE = 2\n",
    "    OUTLIER = 3\n",
    "    MAJORITY = 4\n",
    "    insTypeTheta = {\n",
    "        0:0.5,\n",
    "        1:0.75,\n",
    "        2:1.0+1e-5,\n",
    "        3:0.0-1e-5,\n",
    "    }\n",
    "\n",
    "class MLSOL():\n",
    "    def __init__(self, numOfNeighbors = 5, ratio = 0.1, randomState = 1):\n",
    "        self._weights = []\n",
    "        self._C = [] # C[][] the C_ij for majority class is null\n",
    "        self._insTypes = [] # insTypes[][]\n",
    "        self._knnIndices = [] # type [][]\n",
    "        self._minLabels = []\n",
    "        self._labelIndices = []\n",
    "        self._featureIndices = []\n",
    "        self._sumW = None\n",
    "        self._numOfNeighbors = numOfNeighbors\n",
    "        self._percentageGeneratedInstance = ratio\n",
    "        self._randomState = randomState\n",
    "\n",
    "    def setRatio(self, p):\n",
    "        self._percentageGeneratedInstance = p\n",
    "    \n",
    "    def getRatio(self):\n",
    "        return self._percentageGeneratedInstance\n",
    "\n",
    "    def setRandomState(self, randomState):\n",
    "        self._randomState = randomState\n",
    "    \n",
    "    def getRandomState(self):\n",
    "        return self._randomState\n",
    "\n",
    "    def countC1C0(self, y, numLabels):\n",
    "        c1 = [0]*numLabels\n",
    "        c0 = [0]*numLabels\n",
    "\n",
    "        for e in y:\n",
    "            for j in range(numLabels):\n",
    "                if e[j] == 0:\n",
    "                    c0[j]+=1\n",
    "                elif e[j] == 1:\n",
    "                    c1[j]+=1\n",
    "\n",
    "        return c0, c1\n",
    "\n",
    "    def getMinLabels(self, y, numLabels):\n",
    "        c0, c1 = self.countC1C0(y, numLabels)\n",
    "        minLabels = []\n",
    "\n",
    "        for i in range(numLabels):\n",
    "            minLabels.append(1 if c1[i] > c0[i] else 0)\n",
    "        return minLabels\n",
    "\n",
    "    def fit_resample(self, X, y):\n",
    "        rnd = random.Random(self._randomState)\n",
    "        numLabels = y.shape[1]\n",
    "        self._labelIndices = list(range(numLabels))\n",
    "        self._featureIndices = list(range(X.shape[1]))\n",
    "        nData = len(y)\n",
    "        generatedNumberIns = int(nData * self._percentageGeneratedInstance)\n",
    "        self.knnClassifier = KNeighborsClassifier(n_neighbors=self._numOfNeighbors)\n",
    "\n",
    "\t\t# weights=new double[oriNumIns];\n",
    "        self._minLabels = self.getMinLabels(y, numLabels)\n",
    "\t\t\n",
    "\t\t\n",
    "\n",
    "        self._calculate_weight(X, y)\n",
    "        self._initilizeIns_types(X, y)\n",
    "        xNew = X.copy()\n",
    "        yNew = y.copy()\n",
    "        xNewAdd = []\n",
    "        yNewAdd = []\n",
    "        for  i in range(generatedNumberIns):\n",
    "            d = rnd.uniform(0, 1)\n",
    "            centralIndex = -1\n",
    "            s = 0\n",
    "            for j in range(nData):\n",
    "                s+= self._weights[j]\n",
    "                if d<=s:\n",
    "                    centralIndex = j\n",
    "                    break\n",
    "            referenceIndex = self._knnIndices[centralIndex][rnd.randint(0, self._numOfNeighbors - 1)]\n",
    "            # Instance newData=generateSyntheticInstance(ins.get(centralIndex), ins.get(referenceIndex), centralIndex, referenceIndex, rnd);\n",
    "            xNewAddTemp, yNewAddTemp = self._generate_synthetic_instance(X[centralIndex],y[centralIndex], X[referenceIndex], y[referenceIndex], centralIndex, referenceIndex, rnd)\n",
    "            xNewAdd.append(xNewAddTemp)\n",
    "            yNewAdd.append(yNewAddTemp)\n",
    "\n",
    "        return np.concatenate((xNew,xNewAdd)), np.concatenate((yNew,yNewAdd)) # return new MultiLabelInstances(insNew, mlDataset.getLabelsMetaData()); X,y\n",
    "    \n",
    "    def _calculate_weight(self, X, y): \n",
    "        numInstances = len(y)\n",
    "        numLabels = len(self._labelIndices)\n",
    "        self._knnClassifer = KNeighborsClassifier(n_neighbors=self._numOfNeighbors).fit(X, y)\n",
    "        self._knnIndices = [] # knnIndices=new int[nData][numOfNeighbors];\n",
    "        self._C = [] # C=new Double[oriNumIns][numLabels];\n",
    "        for i in range(numInstances):\n",
    "            self._C.append([])\n",
    "            xData = X[i]\n",
    "            yData = y[i]\n",
    "            result = self._knnClassifer.kneighbors([xData], return_distance=False)[0]\n",
    "            for j in range(self._numOfNeighbors):\n",
    "                self._knnIndices.append(result)\n",
    "\n",
    "            for j in range(numLabels):\n",
    "                numMaj = 0\n",
    "                if yData[self._labelIndices[j]] == self._minLabels[j]:\n",
    "                    for k in range(self._numOfNeighbors):\n",
    "                        if yData[self._labelIndices[j]] != y[result[k]][self._labelIndices[j]]:\n",
    "                            numMaj+=1\n",
    "                    self._C[i].append(numMaj*1.0/self._numOfNeighbors)\n",
    "                else:\n",
    "                    self._C[i].append(None)\n",
    "\n",
    "\n",
    "        # //Transform the C to scores\n",
    "        scores = [ [0.0]*numLabels for e in range(numInstances)] # Double scores[][]=new Double[numIns][numLabels];\n",
    "\n",
    "        for j in range(numLabels):\n",
    "            sum=0.0\n",
    "            c=0\n",
    "            for i in range(numInstances):\n",
    "                if self._C[i][j] != None and self._C[i][j] < 1 :\n",
    "                    sum+=self._C[i][j]\n",
    "                    c+=1\n",
    "            if c!=0 and sum != 0.0:\n",
    "                for i in range(numInstances):\n",
    "                    if self._C[i][j] !=None and self._C[i][j] < 1:\n",
    "                        scores[i][j] =  self._C[i][j]/sum\n",
    "        \n",
    "        self._sumW=0.0\n",
    "        self._weights = []\n",
    "        for i in range(numInstances):\n",
    "            self._weights.append(0.0)\n",
    "            for j in range(numLabels):\n",
    "                if scores[i][j] != None:\n",
    "                    self._weights[i] += scores[i][j]\n",
    "            self._sumW+=self._weights[i]\n",
    "\n",
    "    def _initilizeIns_types(self, X, y):\n",
    "        numInstances = len(y)\n",
    "        numLabels = len(self._labelIndices)\n",
    "        self._insTypes = [] # new InstanceType[numIns][labelIndices.length];\n",
    "        for i in range(numInstances):\n",
    "            self._insTypes.append([])\n",
    "            yData = y[i]\n",
    "            for j in range(numLabels):\n",
    "                if yData[self._labelIndices[j]] == self._minLabels[j]:\n",
    "                    if self._C[i][j] < 0.3 :\n",
    "                        self._insTypes[i].append(InstanceType.SAFE)\n",
    "                    elif self._C[i][j] < 0.7:\n",
    "                        self._insTypes[i].append(InstanceType.BORDERLINE)\n",
    "                    elif self._C[i][j] < 1:\n",
    "                        self._insTypes[i].append(InstanceType.RARE)\n",
    "                    else:\n",
    "                        self._insTypes[i].append(InstanceType.OUTLIER)\n",
    "                else:\n",
    "                    self._insTypes[i].append(InstanceType.MAJORITY)\n",
    "\n",
    "\t\t\n",
    "\t\t# //re-analyse the RARE type\n",
    "        flag = True\n",
    "        while flag:\n",
    "            flag = False\n",
    "            for i in range(numInstances):\n",
    "                for j in range(numLabels):\n",
    "                    if self._insTypes[i][j] == InstanceType.RARE:\n",
    "                        for k in self._knnIndices[i]:\n",
    "                            if self._insTypes[k][j]==InstanceType.SAFE or self._insTypes[i][j]==InstanceType.BORDERLINE:\n",
    "                                self._insTypes[i][j]=InstanceType.BORDERLINE\n",
    "                                flag = True\n",
    "                                break\n",
    "\n",
    "    def _generate_synthetic_instance(self,XCentralInstance, YCentralInstance, XReferenceInstance, YReferenceInstance, centralIndex, referenceIndex, rnd):\n",
    "        numFeatures = len(self._featureIndices)\n",
    "        numLabels = len(self._labelIndices)\n",
    "        xNew = XCentralInstance.copy()\n",
    "        yNew = YCentralInstance.copy()\n",
    "        for i in range(numFeatures):\n",
    "            xNew[i] += rnd.uniform(0, 1)*(XReferenceInstance[i]- XCentralInstance[i])\n",
    "        d1 = np.linalg.norm(XCentralInstance - xNew)\n",
    "        d2 = np.linalg.norm(XReferenceInstance - xNew)\n",
    "        cd = 0.5 if d1 == 0 and d2 == 0  else (d1/(d1+d2))\n",
    "        theta = 0.5\n",
    "\n",
    "        for i in range(numLabels):\n",
    "            j = self._labelIndices[i]\n",
    "            if YCentralInstance[j] == YReferenceInstance[j]:\n",
    "                yNew[j] = YCentralInstance[j]\n",
    "            else:\n",
    "                if self._insTypes[centralIndex][i] == InstanceType.MAJORITY:\n",
    "                    temp = (XCentralInstance, YCentralInstance)\n",
    "                    XCentralInstance, YCentralInstance =  (XReferenceInstance, YReferenceInstance)\n",
    "                    XReferenceInstance, YReferenceInstance = temp\n",
    "                    temp = centralIndex\n",
    "                    centralIndex = referenceIndex\n",
    "                    referenceIndex = temp\n",
    "                    cd = 1.0 - cd\n",
    "                theta = InstanceType.insTypeTheta[self._insTypes[centralIndex][i]]\n",
    "                if cd <= theta:\n",
    "                    yNew[j] = YCentralInstance[j]\n",
    "                else:\n",
    "                    yNew[j] = YReferenceInstance[j]\n",
    "        return xNew, yNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampler = MLSOL(numOfNeighbors = 20, ratio = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "x5,y5 = resampler.fit_resample(X_train.toarray(), y_train.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(508, 72) (508, 6)\n"
     ]
    }
   ],
   "source": [
    "print(x5.shape, y5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(391, 6)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 0]\n",
      " [1 0 0 0 0 1]\n",
      " [0 1 0 0 0 1]\n",
      " ...\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(y5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# initialize Binary Relevance multi-label classifier\n",
    "# with an SVM classifier\n",
    "# SVM in scikit only supports the X matrix in sparse representation\n",
    "\n",
    "classifier = BinaryRelevance(\n",
    "    classifier = XGBClassifier(**paramXGBoost),\n",
    "    require_dense = [False, True]\n",
    ")\n",
    "\n",
    "# train\n",
    "classifier.fit(x5, y5)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2079207920792079"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, hamming_loss, f1_score\n",
    "hamloss = hamming_loss(y_test,predictions)\n",
    "hamloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6591    0.5370    0.5918        54\n",
      "           1     0.6061    0.3390    0.4348        59\n",
      "           2     0.6593    0.6250    0.6417        96\n",
      "           3     0.7667    0.7797    0.7731        59\n",
      "           4     0.8864    0.5342    0.6667        73\n",
      "           5     0.7955    0.6034    0.6863        58\n",
      "\n",
      "   micro avg     0.7247    0.5739    0.6406       399\n",
      "   macro avg     0.7288    0.5697    0.6324       399\n",
      "weighted avg     0.7286    0.5739    0.6348       399\n",
      " samples avg     0.6147    0.5668    0.5629       399\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/itn.wildan.dicky/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "class EMLSOL:\n",
    "    def __init__(self, baseMultiLabelLearner = None, mlSampling = MLSOL(), numModels = 5, samplingRatio = 0.3, randomState = 1):\n",
    "        self.baseMultiLabelLearner = baseMultiLabelLearner\n",
    "        self.mlSampling = mlSampling\n",
    "        self.numModels = numModels\n",
    "        self.samplingRatio = samplingRatio\n",
    "        self.randomState = randomState\n",
    "        self.thresholds = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.mlls = []\n",
    "        self.numLabels = y.shape[1]\n",
    "        for i in range(self.numModels):\n",
    "            print(\"Model-\", i+1, \"Sampling\")\n",
    "            mlSamplingCopy = copy.deepcopy(self.mlSampling)\n",
    "            mlSamplingCopy.setRandomState(i+self.randomState)\n",
    "            Xnew, ynew = mlSamplingCopy.fit_resample(X, y)\n",
    "            model = copy.deepcopy(self.baseMultiLabelLearner)\n",
    "            model.fit(Xnew, ynew)\n",
    "            self.mlls.append(model)\n",
    "        print(\"Calculating thresholds\")\n",
    "        self.calculateThresholds(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        nData = len(X)\n",
    "        result = []\n",
    "        for i in range(nData):\n",
    "            bipartition, confidences = self.makePredictionSingleData(X[i])\n",
    "            result.append(bipartition)\n",
    "        return np.asarray(result)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        nData = len(X)\n",
    "        result = []\n",
    "        for i in range(nData):\n",
    "            bipartition, confidences = self.makePredictionSingleData(X[i])\n",
    "            result.append(confidences)\n",
    "        return np.asarray(result)\n",
    "\n",
    "    def makePredictionSingleData(self, x1):\n",
    "        conf = [0]*self.numLabels\n",
    "        for i in range(self.numModels):\n",
    "            a = self.mlls[i].predict_proba(np.asarray([x1]))\n",
    "            confidences = a\n",
    "            print(self.numLabels)\n",
    "            print(confidences)\n",
    "            for j in range(self.numLabels):\n",
    "                conf[j]+=confidences[j]\n",
    "\n",
    "        for j in range(self.numLabels):\n",
    "            conf[j] /= self.numModels\n",
    "\n",
    "        bipartition = []\n",
    "        for j in range(self.numLabels):\n",
    "            bipartition.append(conf[j] >= self.thresholds[j])\n",
    "\n",
    "        return bipartition, conf\n",
    "\n",
    "    def calculateThresholds(self, X, y):\n",
    "        self.thresholds = []\n",
    "        numInstances = y.shape[0]\n",
    "        numLabels = y.shape[1]\n",
    "    \t# thresholdOptimizationMeasures m=measure;\n",
    "    \t# measure=thresholdOptimizationMeasures.None;\n",
    "        \n",
    "        predictConfidences = []#new double [trainingSet.getNumInstances()][trainingSet.getNumLabels()];\n",
    "        for i in range(numInstances):\n",
    "            Xdata, Ydata = X[i], y[i]\n",
    "            bipartition, conf = self.makePredictionSingleData(Xdata)\n",
    "            predictConfidences.append(conf)\n",
    "\n",
    "        for j in range(numLabels):\n",
    "            maxVal = -1000000000000.0\n",
    "            trueLabels = [ data[j]==1 for data in y]\n",
    "            \n",
    "            d = 0.05\n",
    "            while d<1:\n",
    "                predictLabels = [predictConfidences[i][j]>=d for i in list(range(numInstances))]\n",
    "                #Using Fmeasure\n",
    "                value = f1_score(trueLabels, predictLabels, average='macro')\n",
    "                if value > maxVal:\n",
    "                    maxVal = value\n",
    "                    self.thresholds[j] = d\n",
    "                d+=0.05\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampler2 = MLSOL()\n",
    "classifier2 = BinaryRelevance(\n",
    "    classifier = XGBClassifier(**paramXGBoost),\n",
    "    require_dense = [False, True]\n",
    ")\n",
    "emlsol = EMLSOL(mlSampling = resampler2, baseMultiLabelLearner = classifier2, numModels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model- 1 Sampling\n",
      "Model- 2 Sampling\n",
      "Model- 3 Sampling\n",
      "Calculating thresholds\n",
      "6\n",
      "[[0.00392033 0.61693972 0.95425463 0.06847425 0.23165829 0.00269161]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-243-169e353ec85b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memlsol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-241-92650c31bff1>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Calculating thresholds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculateThresholds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-241-92650c31bff1>\u001b[0m in \u001b[0;36mcalculateThresholds\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumInstances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mXdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mbipartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakePredictionSingleData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mpredictConfidences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-241-92650c31bff1>\u001b[0m in \u001b[0;36mmakePredictionSingleData\u001b[0;34m(self, x1)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfidences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mconfidences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "emlsol.fit(X_train.toarray(), y_train.toarray())\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
